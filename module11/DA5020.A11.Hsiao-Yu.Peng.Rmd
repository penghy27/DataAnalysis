---
title: "DA5020.A11.Hsiao-Yu.Peng"
author: "Hsiao-Yu Peng"
date: "2023-11-22"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(readr)
library(caret)
```


# Q1. Load the diabetes dataset “diabetes.csv”, inspect the data and gather any relevant summary statistics.

```{r}
file_path <- "~/Desktop/2023Fall_Syllabus/DA5020/module11/diabetes-1.csv"

data <- read.csv(file_path, header = TRUE, sep = ",")  

glimpse(data)
summary(data)
```

The dataset has dimensions of 768 x 9. The variable data types are primarily "int" (integer) and "dbl" (float). Based on the summary, the min-max range of variables varies. Therefore, we will normalize the variables in Question 2.



# Q2. Normalize the explanatory variables using min-max normalization.

```{r}
# Define a function for min-max normalization
min_max_normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

# min-max normalization
data <- data.frame(lapply(data, min_max_normalize))

print(data)

```

All the explanatory variables is min-max normalized.



# Q3. Split the data into a training set and a test set i.e. perform an 80/20 split; 80% of the data should be designated as the training data and 20% as the test data.

```{r}
set.seed(123)  # Setting seed for reproducibility

# Create an index for the training set (80%) and the test set (20%)
index <- createDataPartition(data$Outcome, p = 0.8, list = FALSE)

# Create training set
train_data <- data[index, ]

# Create test set
test_data <- data[-index, ]

# Display the dimensions of the training and test sets
cat("Training set dimensions:", dim(train_data), "\n")
cat("Test set dimensions:", dim(test_data), "\n")
```

Using the `caret` package's `createDataPartition()` function, we split the data into a training set with dimensions 615 x 9 and a test set with dimensions 153 x 9.



# Q4. Create a function called knn_predict(). The function should accept the following as input: the training set, the test set and the value of k. 
• Implement the logic for the k-nn algorithm from scratch (without using any libraries). The goal of your k-nn algorithm is to predict the Outcome (i.e. whether or not the patient has diabetes) using the explanatory variables.
• The function should return a list/vector of predictions for all observations in the test set.

```{r}
# Function to calculate Euclidean distance between two vectors
euclidean_distance <- function(vec1, vec2) {
  sqrt(sum((vec1 - vec2)^2))
}

# K-NN prediction function
knn_predict <- function(train_data, test_data, k) {
  predictions <- numeric(length = nrow(test_data))

  for (i in 1:nrow(test_data)) {
    # Calculate distances between the test point and all training points
    distances <- apply(train_data[, -ncol(train_data)], 1, function(train_point) euclidean_distance(train_point, test_data[i, -ncol(test_data)]))

    # Find the k-nearest neighbors
    nearest_neighbors <- order(distances)[1:k]

    # Get the outcomes of the k-nearest neighbors
    neighbor_outcomes <- train_data$Outcome[nearest_neighbors]

    # Make a prediction based on majority vote
    predictions[i] <- ifelse(sum(neighbor_outcomes == 1) > sum(neighbor_outcomes == 0), 1, 0)
  }

  return(predictions)
}

# Predict Outcome
predictions <- knn_predict(train_data, test_data, k = 3)
print(predictions)

```



# Q5. Demonstrate that the knn_predict() function works and use it to make predictions for the test set. You can determine a suitable value of k for your demonstration. After which, analyze the results that were returned from the function using a confusion matrix.

```{r}
# Convert predictions to factor with levels 0 and 1
predictions <- as.factor(predictions)
levels(predictions) <- c("1", "0")  

# Convert test_data$Outcome to factor with levels 0 and 1
test_data$Outcome <- as.factor(test_data$Outcome)
levels(test_data$Outcome) <- c("1", "0")  


# Now create the confusion matrix
conf_matrix <- confusionMatrix(predictions, test_data$Outcome)

# Print the confusion matrix
print(conf_matrix)

```
### The confusion matrix structure as follows:
- True Positives (TP): 88
- True Negatives (TN): 28
- False Positives (FP): 24
- False Negatives (FN): 13

### Summary Statistics:
- Accuracy: 0.7582.   
The proportion of correctly classified instances. In this case, 75.82% of the predictions were correct.
- 95% CI (Confidence Interval): (0.6824, 0.8237).  
The confidence interval for the accuracy.
- No Information Rate: 0.6601.   
The accuracy that could be achieved by always predicting the majority class.
- P-Value [Acc > NIR]:0.005655.  
The p-value comparing the accuracy to the no information rate. A small p-value suggests that the model's accuracy is significantly better than random guessing.
- Kappa: 0.4319.       
Kappa statistic measures the agreement between the actual outcomes and the predictions, correcting for chance. A kappa of 0.4019 suggests a fair agreement.

#### Sensitivity, Specificity, and Predictive Values:
-  Sensitivity (True Positive Rate): 0.8713.     
The proportion of actual positive instances correctly identified by the model.   
- Specificity (True Negative Rate): 0.5385.    
The proportion of actual negative instances correctly identified by the model.  
- Positive Predictive Value (Precision): 0.7857.     
The proportion of predicted positive instances that were actually positive.  
- Negative Predictive Value: 0.6829.    
The proportion of predicted negative instances that were actually negative.  

#### Prevalence and Detection Rates:
- Prevalence: 0.6601.    
The proportion of actual positive instances in the dataset.
- Detection Rate: 0.5752.    
The proportion of instances correctly identified by the model (sensitivity).

#### Balanced Accuracy:
- Balanced Accuracy: 0.7049.   
The average of sensitivity and specificity. It considers both positive and negative classes.   <br>

The model has relatively good accuracy, sensitivity, and positive predictive value. Specificity is moderate, suggesting a reasonable ability to correctly identify negative instances.  



# Q6. Repeat question 5 and perform an experiment using different values of k. Ensure that you try at least 5 different values of k and display the confusion matrix from each attempt. Which value of k produced the most accurate predictions?

```{r}
# Define a range of k values to experiment with
k_values <- c(1, 3, 5, 7, 9) 

# Iterate over different values of k
for (k in k_values) {
  # Use knn_predict to make predictions for the test set
  predictions <- knn_predict(train_data, test_data, k = k)

  # Convert predictions to factor with levels 0 and 1
  predictions <- as.factor(predictions)
  levels(predictions) <- c("0", "1")  # Adjust levels if needed

  # Convert test_data$Outcome to factor with levels 0 and 1
  test_data$Outcome <- as.factor(test_data$Outcome)
  levels(test_data$Outcome) <- c("0", "1")  # Adjust levels if needed

  # Create the confusion matrix
  conf_matrix <- confusionMatrix(predictions, test_data$Outcome)

  # Print the confusion matrix for each k value
  cat("Confusion Matrix for k =", k, ":\n")
  print(conf_matrix$overall["Accuracy"])
  cat("\n")
}

```

Based on the accuracy results, we might consider k = 3 as it has the highest accuracy.